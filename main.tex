\documentclass{scrartcl}
%\usepackage{draftwatermark}
%\SetWatermarkText{DRAFT}
%\SetWatermarkScale{5}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epsfig}
\usepackage[right=0.8in, top=1in, bottom=1in, left=0.8in]{geometry}
\usepackage{setspace}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage{dirtytalk}

\newcommand{\zo}{\{0,1\}}
\newcommand{\mzo}{\{-1,+1\}}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\F}{{\mathbb{F}}}
\newcommand{\N}{{\mathbf{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\BO}{{\mathcal{O}}}
\newcommand{\eps}{\epsilon}
\newcommand{\tO}{\tilde{O}}
\newcommand{\Vol}{\mathop\mathrm{Vol}\nolimits}
\newcommand{\Const}{\mathop\mathrm{Const}\nolimits}
\newcommand{\EX}{{\mathbb E}}
\newcommand{\Sur}{\mathop\mathrm{Sur}\nolimits}
\newcommand{\polylog}{\mathop\mathrm{polylog}\nolimits}
\newcommand{\xor}{\oplus}
\newcommand{\conj}[1]{{\overline {#1}}} %% conjugate

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\E}{{\mathbb E}}
\newcommand{\Var}{{\bold {Var}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{algorithm2e}
\usepackage{placeins}
\usepackage{float}
\usepackage{xargs}     
\graphicspath{ {img/} }
% Use more than one optional parameter in a new commands
% 
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

\title{Locality-Sensitive Hashing without false negatives for $\ell_1$}
\subtitle{COMS 6998 Algorithmic Techniques for Massive Data}
\author{Negev Shekel-Nosatzki, Chen-Yu Yang, Parthiban Loganathan}
\date{December 2015}

\restylefloat{table}
\usetikzlibrary{shapes,arrows,positioning,patterns}

\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=8em, aspect=2, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=10em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\section{Introduction}

\subsection{Abstract}
In this article we analyze Rasmus Pagh recent publication \say{Locality-sensitive Hashing without False Negatives} and extend its findings to $\ell_1$. To simplify our analysis - we have focused on integers bounded by range ($[N]^d$) - by defining random bit mappings to create an efficient hashing that guarantees the collision of close points always and rarely collides far points. We achieve this by mapping far points closer but farther than $cr$ to preserve the LSH property. This results in a much faster algorithm (by expectation) than any existing deterministic $\ell_1$ Nearest Neighbor Search algorithm with near-linear space, similar to the results achieved by Pagh.

\subsection{Acknowledgements}
We would like to thank Prof. Alexandr Andoni for assisting us in project guidelines and direction.
\section{Background}

\subsection{Locality-Sensitive Hashing}
In problems like Nearest Neighbor Search, we need a data structure to distinguish between close and far points with near-linear space and sub-linear query time. This motivates Locality-Sensitive Hashing (LSH).

\begin{definition}[informal]
  A \emph{locality-sensitive hash function, LSH} is a random hash function
  $h:\mathbb{R}^d \to U$
  ($h$ drawn from a family $\mathcal{H}$, $U$ some finite set) such that
  \begin{enumerate}
    \item $d(q,p) \leq r \implies \Pr[h(q)=h(p)] = P_1$ is \say{not-so-small}
    \item $d(q,p) > cr \implies \Pr[h(q)=h(p)] = P_2$ is \say{small}
  \end{enumerate}
  Generally, $P_1<P_2$ and we use $n^\rho$ hash tables where:
  \[ \rho = \frac{\log(1/P_1)}{\log(1/P_2)}.\]
\end{definition}

For the construction of LSH for Hamming Space $\{0,1\}^d$ with distance metric $\norm{x-y}_{Ham}=|\{x_i\neq y_i\}|$, we define the hash family, $\mathcal{H} : \{g:\{0,1\}^d \to \{0,1\}^k\}$ as:

\[ g(p) := (h_1(p), h_2(p), \ldots, h_k(p)),\]
where $h_i(p):= p_j \mbox{ for random } j \leftarrow [d]$. It essentially randomly selects $k$ bits from $d$-dimensional point. For NNS, we maintain $L = n^\rho$ such Hamming-LSH tables $g$. We will see how we can always collide close points by choosing correlated hash functions instead of random independent ones in Pagh'16 and how this applies to our construction.

\subsection{LSH without false negatives from Pagh'16}
\begin{theorem*}
[Pagh'16] Given $S \subseteq \{0,1\}^d, c>1$ and $r \in \mathbf{N}$ there exists a data structure with parameter $\rho$ such that:
\begin{itemize}
	\item On query $y \in \{0,1\}^d$ the data structure us guaranteed to return $x \in S$ with $\norm{x-y} < cr$ if there exists $x' \in S$ with $\norm{x'-y} \leq r$.
	\item The size is $O(dn^{1+\rho})$ bits where $n = |S|$.
	\item The expected query times is $O(n^\rho(1+d/w))$, where $w$ is the word length.
	\item Parameter $\rho$ is a function of $n, r$ and $c$ bounded by $\rho \leq \min{\Big(\ln{4}/c + \frac{\log{r}}{\log{n}}, 1/c + \frac{r}{\log{n}}\Big)}$. If $\log{(n)}/cr \in \mathbf{N}$, then $\rho = 1/c$.
\end{itemize}
\end{theorem*}

The important takeaways from Pagh's approach that are relevant to our construction are below:
\begin{itemize}
    \item It's \textit{covering}, meaning that it guarantees collision for $\norm{x-y} \leq r$. ie. $P_1 = 1$. The probability of false negatives is 0.
    \item Far points collide with low probability.
    \item It chooses correlated hash functions instead of selecting them independently to cover all ways in which two points can be $r$ apart.
    \item Performance is almost as good as what we saw in class where we had constant probability of false negatives. The algorithm is efficient if $r = \Theta(\log{n})$.
\end{itemize}

\subsection{Hash function family from Pagh'16}
\begin{equation*}
    \mathcal{H}_{\mathcal{A}} = \{x \mapsto x \wedge a | a \in \mathcal{A}\}
\end{equation*}

where $\mathcal{A} \subseteq \{0,1\}^d$ is a set of bit masks. 
Given a query $q$, we iterate through all $h \in \mathcal{H}_{\mathcal{A}}$ and check for collisions. To generate a covering set $\mathcal{A}$, we define it as:
\begin{equation*}
    \mathcal{A}(m) = \{a(v) | v \in \{0,1\}^{r+1} \setminus \{\mathbf{0}\}\}
\end{equation*}
where $a(v)_i = \langle m(i),v \rangle$ $\forall$ $v \in \{0,1\}^{r+1}$ and $m : \{1,\cdots,d\} \rightarrow \{0,1\}^{r+1}$ is a random uniformly selected function.

For example, this procedure produces the covering set in \ref{pagh-covering} when $m(i)$ is the binary representation of $i$. Each column represents an $a \in \mathcal{A}$ where 1 samples the coordinate in the point. In Fig. \ref{pagh-covering}, $r=2$ and we see that for any 2 coordinates we select, there exists a column where both coordinates are 1s. This ensures that we can always sample the 2 coordinates that differ between points which are 2-apart.

It is sufficient for any one column has the collision. We could always achieve vectors $a$ that select all $r$ possible coordinates by having ${d \choose r}$ possible values for $a$ but Pagh's procedure produces an efficient covering set with fewer columns.

\begin{figure}
  \centering
        \[ \left( \begin{array}{ccccccc}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        1 & 1 & 0 & 0 & 1 & 1 & 0 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1 \\
        1 & 0 & 1 & 1 & 0 & 1 & 0 \\
        0 & 1 & 1 & 1 & 1 & 0 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
        \end{array} \right)\]
        \caption{Sample $\mathcal{A}$ for $d = 7$ and $r=2$}
        \label{pagh-covering}
  \label{fig:gull}
\end{figure}

LSH is fast only when the input is in Hamming space. It is possible to extend the algorithm to the $\ell_1$ norm by embedding $\ell_1$ into Hamming space. However, it increases the query time and error and adds complexity to the algorithm. We will attempt to extend Pagh's work to $\ell_1$ in a manner that is more efficient than simply embedding $\ell_1$ to Hamming and then running Pagh's algorithm.

\subsection{Other approaches}
In Datur-Immorlica-Indyk-Mirrokni'04, an LSH scheme based on $p$-stable distributions is proposed. It works for any $\ell_p$ norm $p \in (0, 2]$ which includes $\ell_1$ that we use in our attempt. They show that there exists an
algorithm for $(R,c)$-NN which uses $O(dn + n^{1+\rho})$ space and $O(n^\rho log_{1/\delta}{n})$ query time.


% end Background

\section{Main Theorem}
Our main theorem for this project:
\begin{theorem}
Given $S \subseteq [N]^d, c>1$ and $r \in \mathbf{N}$ there exists a data structure with parameter $\rho$ such that:
\begin{itemize}
	\item On query $y \in [N]^d$ the data structure is guaranteed to return $x \in S$ with $\norm{x-y} < cr$ if there exists $x' \in S$ with $\norm{x'-y} \leq r$.
	\item The size is $\BO(r\cdot \log{N} \cdot (N+ d \cdot n^{1+\rho})$ bits where $n = |S|$.
	\item The expected query times is $\BO((r\cdot \log{N} \cdot d)/w \cdot n^{\rho})$, where $w$ is the word length.
	\item Parameter $\rho$ is a function of $n, r$ and $c$ bounded by:
	$$\rho \leq \min{\Big(\ln{4}/c + \frac{\log{r}}{\log{n}}, 1/c + \frac{r}{\log{n}}\Big)}$$
	 If $\log{(n)}/cr \in \mathbf{N}$, then $\rho = 1/c$.
\end{itemize}
\end{theorem}

Note that this is very similar results to Pagh, with another $r\log(N)$ factor as a result from the $\ell1$ domain and our construction. Similarly to Pagh, our algorithm will achieve similar features like finding all close points, etc.
Furthermore, we will also show how we can exchange the added $r$ factor with a $d$ factor, with some constant factor to the denominator and the exponent (trade-off).


\section{Construction}
\subsection{Distance-preserving Mappings}
The main idea behind our extension is to create mappings from $\ell1$ to Hamming that will distinguish between close points and far points. An example of such mapping would be unary embedding which maintains exact distance but is highly inefficient as it grows the dimension by factor N (the range of target domain). In our project we explored more efficient mappings which we called distance-preserving mappings.

\begin{definition}[Distance-Preserving Mapping]
	Define $f : \N \rightarrow \zo^\alpha$ to be $\beta$ distance-preserving $\alpha$-bit mapping, if $\forall x, y \in \N \st |x-y|\leq\beta : \norm{f(x)-f(y)}_{Ham} = |x-y|$
\end{definition}

As an example, the following function is $\alpha$ distance-preserving $\alpha$-bit mapping:

\[
f(x) =
\begin{cases}
    0^{\alpha-\beta}1^\beta,& \text{if } \beta < \alpha\\
    1^{2\alpha-\beta}0^{\beta-\alpha},              & \text{otherwise}
\end{cases}
\]
Where $\beta = x \pmod {2\alpha}$.
\paragraph{}

We are interested in random distance preserving mappings so we can have far points remain far on expectation.

\paragraph{}

Let $\tau : \N \rightarrow [\alpha]$ be the following random function that maps integers to coordinates:
\[
\tau(x) =
\begin{cases}
    x,& \text{if } x \leq \alpha/2\\
    \mathcal{U}([\alpha] \setminus \bigcup_{i=1}^{\alpha/2}\tau(x-i)),              & \text{otherwise}
\end{cases}
\]

($\tau(x)$ simply samples a uniformly random coordinate which wasn't sampled in the previous $\alpha/2$ rounds.)\newline\newline
And let $f : \N \rightarrow \zo^\alpha$ be the following random mapping:

\[
f(x) =
\begin{cases}
    f(0) &= \vec{0} \\
    f(x) &= f(x-1) \xor e_{\tau(x)}
\end{cases}
\]

($f(x)$ simply "flips" the $\tau(x)$ coordinate from $f(x-1)$)
\begin{claim}
f is $\alpha/2$ distance-preserving $\alpha$-bit mapping	
\end{claim}

\begin{proof}
Define $d(x,y) =  \norm{f(x)-f(y)}_{Ham}$.\newline
	Since $\forall x\in \N, i \in [\alpha/2], \tau(x) \neq \tau(x-i)$, then:
	$$\forall j \in [\alpha/2], d(x+j,x) = \norm{(\sum_{i=1}^j e_{\tau(x+i)}) \xor x - x}_{Ham} = j$$
\end{proof}

\paragraph{}


\begin{lemma}[Lower bound on distance expectation]
\label{dist_lb}
	$\forall x,y \st |x-y|\geq \alpha/2 : \E_f[d(x,y)] \geq \alpha/4$
\end{lemma}

\begin{proof}
	We will proof it by induction. Base case: $|x-y|=\alpha/2 \Rightarrow d(x,y)=\alpha/2$ holds from the claim. To calculate general case, we have:
	 \begin{align*}\E[d(x, y)] &= \E[d(x - 1, y) + \Pr[f(x-1)_{\tau(x)} = f(y)_{\tau(x)}] - \Pr[f(x-1)_{\tau(x)} \neq f(y)_{\tau(x)}]] \\
	 &= \E[d(x - 1, y)] + 1 - 2\Pr[f(x-1)_{\tau(x)} \neq f(y)_{\tau(x)}]
\end{align*}
For this we first need to calculate how many matching coordinates were flipped in the previous $\alpha/2$ rounds. Assume that in the previous $\alpha/2$ rounds, we flipped $k$ matching coordinates: $k=|\{f(x - 1 - \alpha/2)_{\tau(x-i)} = f(y)_{\tau(x-i)}); i \in [\alpha/2]\}|$ and $(\alpha/2 - k)$ un-matching coordinates ($f(x - 1 - \alpha/2)_{\tau(x-i)} \neq f(y)_{\tau(x-i)}$). So we have $d(x-1, y) = d(x-1-\alpha/2) + k - (\alpha/2 - k)$ or $k = \frac{d(x-1,y)-d(x-1-\alpha/2,y) + \alpha/2}{2}$.
	
Now we can calculate $\Pr[f(x-1)_{\tau(x)} \neq f(y)_{\tau(x)}]$. From $f(x-1)$ to $f(x)$, there are $d(x-1, y)$ unmatched coordinates, with $k$ coordinates which have been flipped in the previous $\alpha / 2$ rounds. Thus:
\begin{align*}
\Pr[f(x-1)_{\tau(x)} \neq f(y)_{\tau(x)}]
&= \frac{d(x-1,y) - k}{\alpha/2} \\
&= \frac{d(x-1,y) - \frac{d(x-1,y)-d(x-1-\alpha/2,y) + \alpha/2}{2}}{\alpha/2} \\
&= \frac{d(x-1,y)+d(x-1-\alpha/2,y) - \alpha/2}{\alpha}
\end{align*}

And our expectation:
\begin{align*}
\E[d(x,y)]
&= \E[d(x-1,y)] + 1 - 2\Pr[f(x-1)_{\tau(x)} \neq f(y)_{\tau(x)}] \\
&= \E[d(x-1,y)] + 1-2\frac{\E[d(x-1,y)]+\E[d(x-1-\alpha/2,y)] - \alpha/2}{\alpha} \\
&= \E[d(x-1,y)] + 2\left(1-\frac{\E[d(x-1,y)]+\E[d(x-1-\alpha/2,y)]}{\alpha}\right) \\
&= 2+\frac{(\alpha-2)\E[d(x-1,y)] - 2\E[d(x-1-\alpha/2,y)]}{\alpha} \\
&\geq 2+\frac{(\alpha-2)\E[d(x-1,y)] - 2(\E[d(x-1,y)] + \alpha/2)}{\alpha} \\
&= 1 + \frac{(\alpha-4)}{\alpha}\E[d(x-1,y)] \\
&\geq 1+\frac{(\alpha-4)}{\alpha}\cdot\frac{\alpha}{4} = 1 + \frac{\alpha}{4} - 1 = \alpha/4
\end{align*}

Last step is by induction.
\end{proof}


\begin{observation}
This bound is not tight - it is possible to show tighter bounds around $\alpha/2$ with a bit more work - but for our purpose it makes no asymptotic difference so $\alpha/4$ is good enough.
\end{observation}
%&\geq 2+\frac{(\alpha-2)(\alpha/2(1 - \epsilon)) - 2(\alpha/2(1+\epsilon))}{\alpha} \geq \alpha/2(1 - \epsilon) \\
%&\leq 2+\frac{(\alpha-2)(\alpha/2(1 + \epsilon)) - 2(\alpha/2(1-\epsilon))}{\alpha} \leq \alpha/2(1 + \epsilon)


\subsection{Algorithm}

We propose the following algorithm for our $\ell1$ LSH:
\begin{enumerate}
	\item Prepare $K=100\log(N)$ random $\alpha/2$ distance-preserving $\alpha$-bit mappings denoted $f^1..f^K$ (as specified above) with $\alpha=8cr$. We define $F : [N] \rightarrow \zo^{K\alpha}$ to be the concatenation of $f^1..f^K$
	\item Map each $x \in S \subseteq [N]^d$ to $x' \in S' \subseteq \zo^{\alpha K d}$ using mapping T, where $T : [N]^d \rightarrow \zo^{\alpha K d}$ is defined to be the concatenation of $F(x_1)..F(x_d)$
	\item Hash all $x' \in S'$ using Pagh construction
\end{enumerate}

On each NNS query $y \in [N]^d$, we will map $y'=T(y)$, look for nearest-neighbors for $y'$ in $S'$ using Pagh, and for each such neighbor $x'\in S'$, compare the original $\ell1$ distance $\norm{x-y}_1$.

% end Construction

\subsection{Analysis}
\subsubsection{Correctness}
To prove our main theorem, we would like to show that close points remain close always and all far points will remain far with constant probability. As a direct result - Pagh data-structure guarantee of finding close point always with sub-linear expected running time will hold.
\begin{claim}
	For all $x,y \in [N]^d \st \norm{x-y}_1 \leq r$, $\norm{T(x)-T(y)}_{Ham} = K\norm{x-y}_1 \leq Kr$ with probability 1
\end{claim}
\begin{proof}
	Since $\forall i \in [d] : |x_i-y_i|\leq r < 4cr$, then $\norm{T(x)-T(y)}_{Ham} = K\cdot \norm{x-y}_1 \leq Kr$
\end{proof}

\begin{claim}
	For all $x,y \in [N]^d \st \norm{x-y}_1 \geq cr$, $\norm{T(x)-T(y)}_{Ham} \geq Kcr$ (with 90\% probability)
\end{claim}
\begin{proof}
If $\forall i \in [d] : |x_i-y_i|\leq 4cr$, then $\norm{T(x)-T(y)}_{Ham} = K\cdot \norm{x-y}_1 \geq Kcr$. \newline
Otherwise according to Lemma $\ref{farcoord}$ (assuming $i$ is dimension with higher distance):
$$\norm{T(x)-T(y)}_{Ham} \geq \norm{F(x_i)-F(y_i)}_{Ham} \geq Kcr$$
\end{proof}

\begin{lemma}
\label{farcoord}
	$\Pr[\forall a,b \in [N] \st |a-b|\geq 4cr$, $\norm{F(a)-F(b)}_{Ham} \geq Kcr] \geq 0.9$
\end{lemma}
\begin{proof}
By Lemma \ref{dist_lb}, for each $\alpha/2$ distance-preserving function $f^j, j \in [K]$, and for any $a,b \st |a-b| \geq 4cr$:
	$$\E[\norm{f^j(a) - f^j(b)}_{Ham}] \geq \alpha/4 = 2cr$$
	And since the functions are chosen iid, by applying Chernoff:
	\begin{align*}
	\Pr[\norm{F(a) - F(b)}_{Ham} < Kcr] &= \Pr[\sum_{j=1}^K\norm{f^j(a) - f^j(b)}_{Ham}/8cr < 1/2 \cdot K/4] \\
	&\leq exp\left(-\frac{K}{32}\right) < exp\left(-3\log N \right) < 0.1/N
	\end{align*}
	Therefore, by union bound on all distances $|a-b|$:
	$$\Pr[\exists a,b \in [N] \st |a-b|\geq 4cr \text{ and } \norm{F(a)-F(b)}_{Ham} < Kcr] < N \cdot 0.1/N < 0.1$$
\end{proof}
\begin{observation}
We can amplify such probability to $1-\delta$ with another $\log(1/\delta)$ factor in $K$.	
\end{observation}

\subsubsection{Complexity}
As this is a theoretical paper, we were mainly concerned with asymptotic complexity. The final dimension is:
$$d' = \alpha K d = \BO(r\cdot \log{N} \cdot d)$$
\paragraph{}
The space required is the Hashing space plus the space to store the K random functions ($\BO(rN\log N)$):
$$S = \BO(r\cdot \log{N} \cdot (N+ d \cdot n^{1+\rho}) \text{ bits}$$
Note that the space to store the random functions can be absorbed asymptotically whenever $N = \BO(d\cdot n^{1+\rho})$ which is generally the case.
\paragraph{}
To make time analysis comparable to other algorithms, we will assume word-length $w=\log N$:
$$T = \BO((r\cdot \log{N} \cdot d)/w \cdot n^{\rho}) = \BO(r \cdot d \cdot n^{\rho})$$
% end Analysis

\subsection{Replacing the r factor with d}
Note that our final dimension is linear in $r$, which works well with Pagh's Hamming LSH which is optimal when $r = \Theta(\log n)$ or $r=\BO(1)$.
\paragraph{}
It is possible, however, to replace the r factor with d and pay some constant factor in denominator and exponent (trade-off) dividing each coordinate by $\tfrac{\epsilon r}{d}$ (for some constant $\epsilon < c - 1$) and round them to the nearest integer.\newline
As a result, we may "lose" $\tfrac{\epsilon r}{d}$ distance per coordinate and in total $\epsilon r$, so $cr$-far points might only be $(c-\epsilon)r$-far, and our updated parameters:
\begin{align*} 
c'&=c-\epsilon \\
K &= 100\log(\frac{Nd}{\epsilon r}) \\
\alpha &= \frac{8cd}{\epsilon} \\
r' &= K\frac{d}{\epsilon}
\end{align*}

Final dimension becomes: 
$$d' = \BO(d^2\log(\frac{Nd}{\epsilon r})/\epsilon)$$ 

And parameter $\rho$:
$$\rho' \approx \rho + \epsilon/c$$




\section{Conclusion and Future Work}
In this project we analyzed the Pagh construction for guaranteed LSH close-point finding with expected efficiency and extended such findings efficiently to $\ell1$ by introducing random distance-preserving mappings. 
\paragraph{}
Some of the interesting features about our construction:
\begin{itemize}
	\item It can find all close points.
	\item It can be extended to $\ell2$ by $\ell2 \rightarrow \ell1$ embedding.
	\item While it is for range-bounded $\ell1$ ($[N]^d$), it is dynamic in the sense that if the range increases, the random functions can be adjusted for the increased range without needing to rehash all points.
\end{itemize}
There are several ways that our suggested algorithm can be further improved:
\begin{itemize}
	\item While the space to store K random distance preserving functions can generally be absorbed by the model, it might be possible to choose pseudo-random distance-preserving functions to further improve space-efficiency in the worst-case.
	\item Furthermore - it might be possible to find \say{covering} set distance-preserving functions which would:
		\begin{enumerate}
			\item Save space as above.
			\item Remove dependency on $\log(N)$.
			\item Remove range restriction (so would work on $\N^d$).
		\end{enumerate}
	\item Last - some of our bounds can be tightened to for constant improvements in final dimension.
\end{itemize}
% end Conclusion


\section{Citations}
\begin{itemize}
    \item Pagh, Rasmus. "Locality-sensitive hashing without false negatives." arXiv preprint arXiv:1507.03225 (2015).
    \item Indyk, Piotr, and Rajeev Motwani. "Approximate nearest neighbors: towards removing the curse of dimensionality." Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM, 1998.
    \item Datar, Mayur, et al. "Locality-sensitive hashing scheme based on p-stable distributions." Proceedings of the twentieth annual symposium on Computational geometry. ACM, 2004.
\end{itemize}

% end Citations

\FloatBarrier


\end{document}
